{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urCxopINsd1Y"
      },
      "source": [
        "# (Prompt base + Image caption) model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6NnTsmGogJ3"
      },
      "outputs": [],
      "source": [
        "!pip install -q openprompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srspxa0KxL4e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2ryrDaLs5TO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMd5aFKSs5c-"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# stop words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "sww = sw.words('english')\n",
        "# word tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "# stemming\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer=SnowballStemmer(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMJEEKclvGnW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "#import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader,SubsetRandomSampler\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "from openprompt import PromptDataLoader\n",
        "from torch import nn\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryju9_EjaU5M"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "rand_seed = int(time.time())\n",
        "\n",
        "print(\"seed:\", rand_seed)\n",
        "def seed_torch(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "seed_torch(rand_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aMhXJA9svLm"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV5TNr82O2ZS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import csv\n",
        "import torch\n",
        "import pandas as pd\n",
        "# Authenticate\n",
        "drive = None\n",
        "def authenticate():\n",
        "    global drive\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "    authenticate()\n",
        "    for fileId in fileIds:\n",
        "        downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
        "        downloaded.GetContentFile(fileId[0])\n",
        "#Download file if not existing\n",
        "try:\n",
        "  _ = open(\"val_data.csv\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"val_data.csv\", \"1rRiYQwe4pfwS8uT8Jwh9fPfLk2PZfCwW\"], [\"train_data.csv\", \"1N5RSWBIO2FteeZn2oPhiAcgakWSNV-ot\"], [\"test_data.csv\", \"1m4JtSSkwHwkBZM6rIZQ4znBfGBQlSvkp\"]])\n",
        "# https://drive.google.com/file/d/1rRiYQwe4pfwS8uT8Jwh9fPfLk2PZfCwW/view?usp=share_link\n",
        "# https://drive.google.com/file/d/1N5RSWBIO2FteeZn2oPhiAcgakWSNV-ot/view?usp=share_link\n",
        "# https://drive.google.com/file/d/1m4JtSSkwHwkBZM6rIZQ4znBfGBQlSvkp/view?usp=share_link\n",
        "\n",
        "try:\n",
        "  _ = open(\"text_b.csv\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"text_b.csv\", \"1-Ost-ExFIubQJYBQDxmMUd1F0DiXh5QN\"]])\n",
        "# https://drive.google.com/file/d/1-Ost-ExFIubQJYBQDxmMUd1F0DiXh5QN/view?usp=share_link\n",
        "\n",
        "val_dataset = pd.read_csv(\"/content/val_data.csv\")\n",
        "train_dataset = pd.read_csv(\"/content/train_data.csv\")\n",
        "test_dataset = pd.read_csv(\"/content/test_data.csv\")\n",
        "\n",
        "# text_b contains results from vision_in_text\n",
        "text_b_dataset = pd.read_csv(\"text_b.csv\")\n",
        "text_b_dataset = text_b_dataset.dropna()\n",
        "text_b_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_gTVoOS8pBr"
      },
      "outputs": [],
      "source": [
        "label_texts = train_dataset['labels'].unique()\n",
        "print(label_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqTHFbVQtIqg"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqwa3WE9BgKy"
      },
      "outputs": [],
      "source": [
        "label_dict = {}\n",
        "for id, lab in enumerate(label_texts):\n",
        "  label_dict[lab] = id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml8Y-wHCcBsd"
      },
      "outputs": [],
      "source": [
        "top_k = 1\n",
        "\n",
        "datasets = [val_dataset, train_dataset, test_dataset]\n",
        "\n",
        "for i,dataset in enumerate(datasets):\n",
        "  dataset['labels'] = [label_dict[l] for l in dataset['labels']]\n",
        "  datasets[i] = pd.merge(dataset, text_b_dataset, on='id')\n",
        "  datasets[i]['text_z'] = [','.join(text.split(sep=',')[1:top_k+1]) for text in datasets[i]['text_y']]\n",
        "  datasets[i]['text_y'] = [text.split(sep=',')[0] for text in datasets[i]['text_y']]\n",
        "\n",
        "val_dataset = datasets[0]\n",
        "train_dataset = datasets[1]\n",
        "test_dataset = datasets[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CzKUzvPtWKG"
      },
      "source": [
        "# Open prompt structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJVFmQ4mtcVm"
      },
      "source": [
        "##1.Define a task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5508xylpRiT"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define a task\n",
        "\n",
        "from openprompt.data_utils import InputExample\n",
        "\n",
        "def get_prompt_dataset(text_a, text_b, label_digit):\n",
        "  dataset = []\n",
        "  for t,j,i in zip(text_a, text_b, label_digit):\n",
        "      t = str(t).lower()\n",
        "      j = str(j).lower()\n",
        "      a = InputExample(text_a = t,text_b = j, label = int(i))\n",
        "      dataset.append(a)\n",
        "  return dataset\n",
        "\n",
        "def get_new_prompt_dataset(text_a, text_b, text_c, label_digit):\n",
        "  dataset = []\n",
        "  for t,j,h,i in zip(text_a, text_b, text_c, label_digit):\n",
        "      t = str(t).lower().strip('\\n')\n",
        "      j = str(j).lower().strip('\\n')\n",
        "      a = InputExample(text_a = t,text_b = j, meta={\"text_c\":h}, label = int(i))\n",
        "      dataset.append(a)\n",
        "  return dataset\n",
        "\n",
        "val_prompt_dataset = get_new_prompt_dataset(val_dataset['text_x'], val_dataset['text_y'], val_dataset['text_z'], val_dataset['labels'])\n",
        "train_prompt_dataset = get_new_prompt_dataset(train_dataset['text_x'], train_dataset['text_y'], train_dataset['text_z'], train_dataset['labels'])\n",
        "test_prompt_dataset = get_new_prompt_dataset(test_dataset['text_x'], test_dataset['text_y'], test_dataset['text_z'], test_dataset['labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q57KZpe2tlII"
      },
      "source": [
        "##2.Define a Pre-trained Language Models (PLMs) as backbone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTieyZjYpg3h"
      },
      "outputs": [],
      "source": [
        "# Step 2: Define a Pre-trained Language Models (PLMs) as backbone.\n",
        "# 第二步：选择预训练模型\n",
        "from openprompt.plms import load_plm\n",
        "\n",
        "model_name = [\"t5\",\"t5-base\"]\n",
        "\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(model_name[0], model_name[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II6vzapztulG"
      },
      "source": [
        "##3.Define a **Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VlEu9clpqE1"
      },
      "outputs": [],
      "source": [
        "# Step 3: Define a Template.\n",
        "# 第三步：定义模板（Template）\n",
        "from openprompt.prompts import ManualTemplate, MixedTemplate\n",
        "\n",
        "promptTemplate = ManualTemplate(\n",
        "    text='The tweet is {\"placeholder\":\"text_a\"}, {\"placeholder\":\"text_b\"}. So the meme is {\"mask\"}.',\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "prompt_2_template = MixedTemplate(\n",
        "    text='The tweet is {\"placeholder\":\"text_a\"}, {\"placeholder\":\"text_b\"}. So the effect range is {\"mask\"}.',\n",
        "    tokenizer=tokenizer,\n",
        "    model=plm\n",
        ")\n",
        "\n",
        "prompt_3_template = MixedTemplate(\n",
        "    text='Tweet text: {\"placeholder\":\"text_a\"} Caption: {\"placeholder\":\"text_b\"} Keywords: {\"meta\":\"text_c\"} Is it harmful? Answer: {\"mask\"}.',\n",
        "    tokenizer=tokenizer,\n",
        "    model=plm\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRCh-6RBt_CQ"
      },
      "source": [
        "##4.Define a Verbalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glbOtL_OZXV3"
      },
      "outputs": [],
      "source": [
        "# Step 4: Define a Verbalizer\n",
        "# 第四步：定义映射（Verbalizer）\n",
        "from openprompt.prompts import ManualVerbalizer\n",
        "\n",
        "promptVerbalizer = ManualVerbalizer(\n",
        "  num_classes=len(label_texts),\n",
        "  label_words=[[l] for l in label_texts],\n",
        "  tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5DhvQSGgimm"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoUyMRqnvSTY"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcUqmqRoErrL"
      },
      "outputs": [],
      "source": [
        "from openprompt import PromptForClassification\n",
        "\n",
        "# Hyper Parameters\n",
        "\n",
        "train_batch_size = 30\n",
        "val_batch_size = 50\n",
        "max_seq_length = 256\n",
        "learning_rate = 1e-4\n",
        "epoch_num = 5\n",
        "print(\"top_k:\", top_k)\n",
        "print(\"seed:\", rand_seed)\n",
        "print(\"model name:\", model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrVe2W1Hu-ar"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from openprompt import PromptForClassification\n",
        "from tqdm import tqdm\n",
        "\n",
        "best_model = None\n",
        "\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(model_name[0], model_name[1])\n",
        "prompt_model = PromptForClassification(plm=plm, template=prompt_3_template, verbalizer=promptVerbalizer, freeze_plm=False)\n",
        "prompt_model = prompt_model.to(device)\n",
        "#load dataset\n",
        "train_loader = PromptDataLoader(dataset=train_prompt_dataset, template=prompt_3_template, tokenizer=tokenizer,\n",
        "  tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_length, decoder_max_length=3,\n",
        "  batch_size=train_batch_size,shuffle=True, teacher_forcing=False, predict_eos_token=False)\n",
        "\n",
        "val_loader = PromptDataLoader(dataset=val_prompt_dataset, template=prompt_3_template, tokenizer=tokenizer,\n",
        "  tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_length, decoder_max_length=3,\n",
        "  batch_size=val_batch_size,shuffle=False, teacher_forcing=False, predict_eos_token=False)\n",
        "test_loader = PromptDataLoader(dataset=test_prompt_dataset, template=prompt_3_template, tokenizer=tokenizer,\n",
        "  tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_length, decoder_max_length=3,\n",
        "  batch_size=val_batch_size,shuffle=False, teacher_forcing=False, predict_eos_token=False)\n",
        "\n",
        "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "\n",
        "optimizer_grouped_parameters = [\n",
        "      {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "      'weight_decay': 0.01},\n",
        "      {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "      'weight_decay': 0.0}]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "\n",
        "\n",
        "# Train process\n",
        "fold_preds = []\n",
        "fold_labels = []\n",
        "best_f1 = 0\n",
        "for epoch in range(epoch_num):\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  prompt_model.train()\n",
        "  t_n=len(train_loader)\n",
        "\n",
        "  train_preds = []\n",
        "  train_labels = []\n",
        "  for step, inputs in enumerate(tqdm(train_loader)):\n",
        "      inputs = inputs.cuda()\n",
        "      logits = prompt_model(inputs)\n",
        "      labels = inputs['label']\n",
        "      loss = loss_func(logits, labels)\n",
        "      loss.backward()\n",
        "      train_loss += loss.item()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      train_labels.extend(labels.cpu().tolist())\n",
        "      train_preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "\n",
        "      acc = sum([int(i==j) for i,j in zip(train_preds, train_labels)])/len(train_preds)\n",
        "      train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
        "\n",
        "      train_acc+= acc\n",
        "  epoch_loss = train_loss /t_n\n",
        "  epoch_acc = train_acc/t_n\n",
        "  print('Epoch: %d, train_loss: %.5f, train_acc: %.5f'%(epoch+1,epoch_loss,epoch_acc))\n",
        "\n",
        "  # validation process\n",
        "\n",
        "  with torch.no_grad() :\n",
        "    prompt_model.eval()\n",
        "    val_tloss = 0\n",
        "    val_n=len(test_loader)\n",
        "\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    for step, inputs in enumerate(val_loader):\n",
        "        inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)\n",
        "        labels = inputs['label']\n",
        "        loss = loss_func(logits, labels)\n",
        "        val_loss = loss.item()\n",
        "        val_tloss+= val_loss\n",
        "\n",
        "        val_labels.extend(labels.cpu().tolist())\n",
        "        val_preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "        vloss = val_tloss/val_n\n",
        "\n",
        "    val_acc = accuracy_score(np.array(val_preds),np.array(val_labels))\n",
        "    val_pre,val_rec,val_f1,_=precision_recall_fscore_support(np.array(val_preds),np.array(val_labels),average='macro')\n",
        "\n",
        "    vloss = val_tloss/val_n\n",
        "    print('val_loss: %.5f, val_acc: %.4f, val_f1: %.4f, val_precision: %.4f, val_recall: %.4f '%(vloss,val_acc,val_f1,val_pre,val_rec))\n",
        "    if val_f1 > best_f1:\n",
        "      best_f1 = val_f1\n",
        "      print(\"best_f1 updated:\", best_f1)\n",
        "      fold_preds = val_preds\n",
        "      torch.save(prompt_model.state_dict(), './harmp_label_checkpoint_k={}_{}.pth'.format(top_k,rand_seed))\n",
        "\n",
        "with torch.no_grad() :\n",
        "    prompt_model.eval()\n",
        "    test_preds = []\n",
        "    test_labels = []\n",
        "    for step, inputs in enumerate(test_loader):\n",
        "        inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)\n",
        "        labels = inputs['label']\n",
        "        loss = loss_func(logits, labels)\n",
        "        test_labels.extend(labels.cpu().tolist())\n",
        "        test_preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mmae(expected, predicted, classes):\n",
        "    NUM_CLASSES = len(classes)\n",
        "    count_dict = {}\n",
        "    dist_dict = {}\n",
        "    for i in range(NUM_CLASSES):\n",
        "        count_dict[i] = 0\n",
        "        dist_dict[i] = 0.0\n",
        "    for i in range(len(expected)):\n",
        "        dist_dict[expected[i]] += abs(expected[i] - predicted[i])\n",
        "        count_dict[expected[i]] += 1\n",
        "    overall = 0.0\n",
        "    for claz in range(NUM_CLASSES):\n",
        "        class_dist =  1.0 * dist_dict[claz] / count_dict[claz]\n",
        "        overall += class_dist\n",
        "    overall /= NUM_CLASSES\n",
        "#     return overall[0]\n",
        "    return overall"
      ],
      "metadata": {
        "id": "0jl5wlSrhmuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmae = calculate_mmae(test_labels, fold_preds, label_texts)\n",
        "print(\"mmae:\", mmae)"
      ],
      "metadata": {
        "id": "nUo-1nahhsgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"mmae:\", mmae)\n",
        "print(\"model XX \\n harmp_label:\\n\", classification_report(test_labels, fold_preds, digits=4))\n"
      ],
      "metadata": {
        "id": "Hgok4thrhwIR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
