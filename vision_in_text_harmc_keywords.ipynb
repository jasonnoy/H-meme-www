{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urCxopINsd1Y"
      },
      "source": [
        "# (Prompt base + Image caption) model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srspxa0KxL4e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6NnTsmGogJ3"
      },
      "outputs": [],
      "source": [
        "!pip install -q openprompt\n",
        "!pip install -q ftfy regex tqdm\n",
        "!pip install -q git+https://github.com/openai/CLIP.git\n",
        "!pip install -q git+https://github.com/jasonnoy/BLIP.git\n",
        "!git clone https://github.com/jasonnoy/BLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2ryrDaLs5TO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7laeCCqyLAy"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import hashlib\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from models.blip import blip_decoder\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Loading BLIP model...\")\n",
        "blip_image_eval_size = 384\n",
        "blip_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'\n",
        "blip_model = blip_decoder(pretrained=blip_model_url, image_size=blip_image_eval_size, vit='large', med_config='./BLIP/configs/med_config.json')\n",
        "blip_model.eval()\n",
        "blip_model = blip_model.to(device)\n",
        "\n",
        "print(\"Loading CLIP model...\")\n",
        "clip_model_name = 'ViT-L/14' # https://huggingface.co/openai/clip-vit-large-patch14\n",
        "clip_model, clip_preprocess = clip.load(clip_model_name, device=device)\n",
        "clip_model.to(device).eval()\n",
        "\n",
        "chunk_size = 2048\n",
        "flavor_intermediate_count = 256"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading BLIP model...\")\n",
        "blip_image_eval_size = 384\n",
        "blip_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'\n",
        "blip_model = blip_decoder(pretrained=blip_model_url, image_size=blip_image_eval_size, vit='large', med_config='./BLIP/configs/med_config.json')\n",
        "blip_model.eval()\n",
        "blip_model = blip_model.to(device)"
      ],
      "metadata": {
        "id": "hTYKZUWaQruY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aMhXJA9svLm"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV5TNr82O2ZS"
      },
      "outputs": [],
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import csv\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "# Authenticate\n",
        "drive = None\n",
        "def authenticate():\n",
        "    global drive\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "#Download files\n",
        "def downloadFiles(fileIds):\n",
        "    authenticate()\n",
        "    for fileId in fileIds:\n",
        "        downloaded = drive.CreateFile({\"id\": fileId[1]})\n",
        "        downloaded.GetContentFile(fileId[0])\n",
        "#Download file if not existing\n",
        "\n",
        "try:\n",
        "  _ = open(\"harmc_images.zip\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"harmc_images.zip\", \"11hvQxSQSqAPekKwHoE4WlC1KgXgeYWru\"]])\n",
        "try:\n",
        "  _ = open(\"parts.zip\", \"r\")\n",
        "except:\n",
        "  downloadFiles([[\"parts.zip\", \"1ydtbt8jIA0PFQHIV8Ee4L93Vt1FxmC-d\"]])\n",
        "# https://drive.google.com/file/d/1ydtbt8jIA0PFQHIV8Ee4L93Vt1FxmC-d/view?usp=share_link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtyHHH4j00mX"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  _ = open(\"/content/images/covid_memes_2.png\", \"r\")\n",
        "except:\n",
        "  !unzip -q harmc_images.zip\n",
        "\n",
        "try:\n",
        "  _ = open(\"/content/flavors.txt\", \"r\")\n",
        "except:\n",
        "  !unzip -q parts.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC-IdMSr4ZMB"
      },
      "outputs": [],
      "source": [
        "image_dirs = os.listdir(\"./images\")\n",
        "image_ids = [f[:-4] for f in image_dirs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t3cOXs4Z7re"
      },
      "outputs": [],
      "source": [
        "imgs={}\n",
        "for i, img_dir in enumerate(image_dirs):\n",
        "  imgs[image_ids[i]] = Image.open(os.path.join(\"./images/\", img_dir)).convert(\"RGB\")\n",
        "print(\"image number:\", len(imgs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZpJR8KgY5Fp"
      },
      "source": [
        "## CLIP-Interrogator\n",
        "by pharmapsychotic\n",
        "opensource: https://huggingface.co/spaces/pharma/CLIP-Interrogator/tree/main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1unBQwH3n5A"
      },
      "outputs": [],
      "source": [
        "class LabelTable():\n",
        "    def __init__(self, labels, desc):\n",
        "        self.labels = labels\n",
        "        self.embeds = []\n",
        "\n",
        "        hash = hashlib.sha256(\",\".join(labels).encode()).hexdigest()\n",
        "\n",
        "        os.makedirs('./cache', exist_ok=True)\n",
        "        cache_filepath = f\"./cache/{desc}.pkl\"\n",
        "        if desc is not None and os.path.exists(cache_filepath):\n",
        "            with open(cache_filepath, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "                if data['hash'] == hash:\n",
        "                    self.labels = data['labels']\n",
        "                    self.embeds = data['embeds']\n",
        "\n",
        "        if len(self.labels) != len(self.embeds):\n",
        "            self.embeds = []\n",
        "            chunks = np.array_split(self.labels, max(1, len(self.labels)/chunk_size))\n",
        "            for chunk in tqdm(chunks, desc=f\"Preprocessing {desc}\" if desc else None):\n",
        "                text_tokens = clip.tokenize(chunk).to(device)\n",
        "                with torch.no_grad():\n",
        "                    text_features = clip_model.encode_text(text_tokens).float()\n",
        "                text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "                text_features = text_features.half().cpu().numpy()\n",
        "                for i in range(text_features.shape[0]):\n",
        "                    self.embeds.append(text_features[i])\n",
        "\n",
        "            with open(cache_filepath, 'wb') as f:\n",
        "                pickle.dump({\"labels\":self.labels, \"embeds\":self.embeds, \"hash\":hash}, f)\n",
        "\n",
        "    def _rank(self, image_features, text_embeds, top_count=1):\n",
        "        top_count = min(top_count, len(text_embeds))\n",
        "        similarity = torch.zeros((1, len(text_embeds))).to(device)\n",
        "        text_embeds = torch.stack([torch.from_numpy(t) for t in text_embeds]).float().to(device)\n",
        "        for i in range(image_features.shape[0]):\n",
        "            similarity += (image_features[i].unsqueeze(0) @ text_embeds.T).softmax(dim=-1)\n",
        "        _, top_labels = similarity.cpu().topk(top_count, dim=-1)\n",
        "        return [top_labels[0][i].numpy() for i in range(top_count)]\n",
        "\n",
        "    def rank(self, image_features, top_count=1):\n",
        "        if len(self.labels) <= chunk_size:\n",
        "            tops = self._rank(image_features, self.embeds, top_count=top_count)\n",
        "            return [self.labels[i] for i in tops]\n",
        "\n",
        "        num_chunks = int(math.ceil(len(self.labels)/chunk_size))\n",
        "        keep_per_chunk = int(chunk_size / num_chunks)\n",
        "\n",
        "        top_labels, top_embeds = [], []\n",
        "        for chunk_idx in range(num_chunks):\n",
        "            start = chunk_idx*chunk_size\n",
        "            stop = min(start+chunk_size, len(self.embeds))\n",
        "            tops = self._rank(image_features, self.embeds[start:stop], top_count=keep_per_chunk)\n",
        "            top_labels.extend([self.labels[start+i] for i in tops])\n",
        "            top_embeds.extend([self.embeds[start+i] for i in tops])\n",
        "\n",
        "        tops = self._rank(image_features, top_embeds, top_count=top_count)\n",
        "        return [top_labels[i] for i in tops]\n",
        "\n",
        "def generate_caption(pil_image):\n",
        "    gpu_image = T.Compose([\n",
        "        T.Resize((blip_image_eval_size, blip_image_eval_size), interpolation=TF.InterpolationMode.BICUBIC),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    ])(pil_image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        caption = blip_model.generate(gpu_image, sample=False, num_beams=3, max_length=20, min_length=5)\n",
        "    return caption[0]\n",
        "\n",
        "def load_list(filename):\n",
        "    with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        items = [line.strip() for line in f.readlines()]\n",
        "    return items\n",
        "\n",
        "def rank_top(image_features, text_array):\n",
        "    text_tokens = clip.tokenize([text for text in text_array]).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    similarity = torch.zeros((1, len(text_array)), device=device)\n",
        "    for i in range(image_features.shape[0]):\n",
        "        similarity += (image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    _, top_labels = similarity.cpu().topk(1, dim=-1)\n",
        "    return text_array[top_labels[0][0].numpy()]\n",
        "\n",
        "def similarity(image_features, text):\n",
        "    text_tokens = clip.tokenize([text]).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "    return similarity[0][0]\n",
        "\n",
        "def interrogate(image):\n",
        "    caption = generate_caption(image)\n",
        "\n",
        "    images = clip_preprocess(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(images).float()\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    flaves = flavors.rank(image_features, flavor_intermediate_count)\n",
        "    best_entity = entities.rank(image_features, 20)[0]\n",
        "    # best_celebrity = celebrities.rank(image_features, 3)[0]\n",
        "    # best_event = events.rank(image_features, 3)[0]\n",
        "    # best_medium = mediums.rank(image_features, 3)[0]\n",
        "    # best_artist = artists.rank(image_features, 3)[0]\n",
        "    # best_trending = trendings.rank(image_features, 3)[0]\n",
        "    # best_movement = movements.rank(image_features, 3)[0]\n",
        "\n",
        "    best_prompt = caption\n",
        "    best_sim = similarity(image_features, best_prompt)\n",
        "\n",
        "    def check(addition):\n",
        "        nonlocal best_prompt, best_sim\n",
        "        prompt = best_prompt + \", \" + addition\n",
        "        sim = similarity(image_features, prompt)\n",
        "        if sim > best_sim:\n",
        "            best_sim = sim\n",
        "            best_prompt = prompt\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def check_multi_batch(opts):\n",
        "        nonlocal best_prompt, best_sim\n",
        "        prompts = []\n",
        "        for i in range(2**len(opts)):\n",
        "            prompt = best_prompt\n",
        "            for bit in range(len(opts)):\n",
        "                if i & (1 << bit):\n",
        "                    prompt += \", \" + opts[bit]\n",
        "            prompts.append(prompt)\n",
        "\n",
        "        prompt = rank_top(image_features, prompts)\n",
        "        sim = similarity(image_features, prompt)\n",
        "        if sim > best_sim:\n",
        "            best_sim = sim\n",
        "            best_prompt = prompt\n",
        "    # check_multi_batch([best_medium, best_artist, best_trending, best_movement])\n",
        "    check_multi_batch([best_entity])\n",
        "    extended_flavors = set(flaves)\n",
        "    for _ in range(20): # Flavor chain\n",
        "        try:\n",
        "            best = rank_top(image_features, [f\"{best_prompt}, {f}\" for f in extended_flavors])\n",
        "            flave = best[len(best_prompt)+2:]\n",
        "            if not check(flave):\n",
        "                break\n",
        "            extended_flavors.remove(flave)\n",
        "        except:\n",
        "            # exceeded max prompt length\n",
        "            break\n",
        "    return best_prompt\n",
        "\n",
        "def inference(image):\n",
        "    return interrogate(image)\n",
        "\n",
        "# sites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\n",
        "# trending_list = [site for site in sites]\n",
        "# trending_list.extend([\"trending on \"+site for site in sites])\n",
        "# trending_list.extend([\"featured on \"+site for site in sites])\n",
        "# trending_list.extend([site+\" contest winner\" for site in sites])\n",
        "\n",
        "# artists = [f\"by {a}\" for a in raw_artists]\n",
        "# artists.extend([f\"inspired by {a}\" for a in raw_artists])\n",
        "\n",
        "# celebrities = LabelTable(load_list('./parts/celebrities.txt'), \"celebrities\")\n",
        "flavors = LabelTable(load_list('./flavors.txt'), \"flavors\")\n",
        "entities = LabelTable(load_list('./entities.txt'), \"entities\")\n",
        "# mediums = LabelTable(load_list('mediums.txt'), \"mediums\")\n",
        "# movements = LabelTable(load_list('movements.txt'), \"movements\")\n",
        "# trendings = LabelTable(trending_list, \"trendings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY3FC-9v6a6v"
      },
      "outputs": [],
      "source": [
        "res_parts = np.array_split(image_ids, 10)\n",
        "for i, part in enumerate(res_parts):\n",
        "  print(\"part\", i)\n",
        "  res = {}\n",
        "  for id in tqdm(part):\n",
        "    try:\n",
        "      res_text = inference(imgs[id])\n",
        "      # print(\"id:{}, text:{}\".format(id, res_text))\n",
        "      res[id] = res_text\n",
        "    except Exception as e:\n",
        "      print(\"id:{}, error:{}\".format(id, e))\n",
        "  res_df = pd.DataFrame(pd.Series(res), columns=['text'])\n",
        "  res_df = res_df.reset_index().rename(columns={'index':'id'})\n",
        "  res_df.to_csv(\"harmc_image_interrogations_20_entity_{}.csv\".format(i))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ucPwBqinkP9"
      },
      "outputs": [],
      "source": [
        "final = pd.DataFrame()\n",
        "for i in range(10):\n",
        "  df = pd.read_csv(\"harmc_image_interrogations_20_entity_{}.csv\".format(i))\n",
        "  final = final.append(df)\n",
        "final = final.reset_index()[['id','text']]\n",
        "final.to_csv(\"harmc_image_interrogations_20_entity_total.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}